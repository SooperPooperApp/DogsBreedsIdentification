{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing and Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import expand_dims\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from skimage.transform import resize\n",
    "from IPython.display import SVG\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import applications\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, BatchNormalization, GlobalAveragePooling2D\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from tensorflow.keras.utils import to_categorical, model_to_dot, plot_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data_dir = \"input/\"\n",
    "img_width, img_height = 224, 224 \n",
    "channels = 3\n",
    "batch_size = 64\n",
    "num_images= 50\n",
    "image_arr_size= img_width * img_height * channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images(image_dir):\n",
    "\n",
    "    image_index = 0\n",
    "    image_arr_size= img_width * img_height * channels\n",
    "    images = np.ndarray(shape=(num_images, image_arr_size))\n",
    "    labels = np.array([])                       \n",
    "\n",
    "    for type in os.listdir(image_dir)[:50]:\n",
    "        type_images = os.listdir(image_dir + type)\n",
    "        labels= np.append(labels, type.split('-')[1])\n",
    "        \n",
    "        for image in type_images[:1]:\n",
    "            image_file = os.path.join(image_dir, type + '/', image)\n",
    "            image_data = mpimg.imread(image_file)\n",
    "            image_resized = resize(image_data, (img_width, img_height), anti_aliasing=True)\n",
    "            images[image_index, :] = image_resized.flatten()\n",
    "#             print (type, ':', image)\n",
    "            image_index += 1\n",
    "\n",
    "    return (images, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale= 1./255,\n",
    "    shear_range= 0.2,\n",
    "    zoom_range= 0.2,\n",
    "    horizontal_flip= True,\n",
    "    rotation_range= 20,\n",
    "    width_shift_range= 0.2,\n",
    "    height_shift_range= 0.2,   \n",
    "    validation_split=0.2,\n",
    "\n",
    ")\n",
    "\n",
    "valid_datagen = ImageDataGenerator(\n",
    "    rescale= 1./255, \n",
    "    validation_split=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_generator = train_datagen.flow_from_directory(  \n",
    "    train_data_dir,  \n",
    "    target_size= (img_width, img_height), \n",
    "    color_mode= 'rgb',\n",
    "    batch_size= batch_size,  \n",
    "    class_mode= 'categorical',\n",
    "    subset='training',\n",
    "    shuffle= True, \n",
    "    seed= 1337\n",
    ") \n",
    "\n",
    "valid_generator = valid_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size= (img_width, img_height),\n",
    "    color_mode= 'rgb',\n",
    "    batch_size= batch_size,  \n",
    "    class_mode= 'categorical',\n",
    "    subset='validation',\n",
    "    shuffle= True, \n",
    "    seed= 1337\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_classes = len(train_generator.class_indices)  \n",
    "train_labels = train_generator.classes \n",
    "train_labels = to_categorical(train_labels, num_classes=num_classes)\n",
    "valid_labels = valid_generator.classes \n",
    "valid_labels = to_categorical(valid_labels, num_classes=num_classes)\n",
    "nb_train_samples = len(train_generator.filenames)  \n",
    "nb_valid_samples = len(valid_generator.filenames)\n",
    "\n",
    "label_map = (list(train_generator.class_indices.keys()))\n",
    "class_file = open(\"classes.txt\", \"w\")\n",
    "\n",
    "# print(label_map)\n",
    "\n",
    "for s in label_map:\n",
    "    print(s)\n",
    "    class_file.write(s +\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "InceptionV3 = applications.InceptionV3(include_top= False, input_shape= (img_width, img_height, channels), weights= 'imagenet')\n",
    "InceptionV3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "for layer in InceptionV3.layers:\n",
    "    layer.trainable= False\n",
    "#     print(layer,layer.trainable)\n",
    "    \n",
    "model.add(InceptionV3)\n",
    "model.add(GlobalAveragePooling2D())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(126,activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer= keras.optimizers.Adam(lr= 0.0001), loss= 'categorical_crossentropy', metrics= ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(\n",
    "    'breeds_model126.h5',\n",
    "    monitor='val_loss',\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    mode='auto',\n",
    "    save_weights_only=False,\n",
    "    period=1\n",
    ")\n",
    "\n",
    "earlystop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0.001,\n",
    "    patience=3,\n",
    "    verbose=1,\n",
    "    mode='auto'\n",
    ")\n",
    "\n",
    "csvlogger = CSVLogger(\n",
    "    filename= \"training_csv.log\",\n",
    "    separator = \",\",\n",
    "    append = False\n",
    ")\n",
    "\n",
    "reduceLR = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.1,\n",
    "    patience=3,\n",
    "    verbose=1, \n",
    "    mode='auto'\n",
    ")\n",
    "\n",
    "callbacks = [checkpoint, earlystop, csvlogger,reduceLR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_generator, \n",
    "    epochs = 30,\n",
    "    steps_per_epoch = nb_train_samples//batch_size,\n",
    "    validation_data = valid_generator, \n",
    "    validation_steps = nb_valid_samples//batch_size,\n",
    "    verbose = 2, \n",
    "    callbacks = callbacks,\n",
    "    shuffle = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(eval_loss, eval_accuracy) = model.evaluate(valid_generator, batch_size= batch_size, verbose= 1)\n",
    "print('Validation Loss: ', eval_loss)\n",
    "print('Validation Accuracy: ', eval_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.subplot()\n",
    "plt.title('Model Accuracy')\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(['Training Accuracy','Validation Accuracy'])\n",
    "plt.savefig('baseline_acc_epoch.png', transparent= False, bbox_inches= 'tight', dpi= 900)\n",
    "plt.show()\n",
    "\n",
    "plt.title('Model Loss')\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(['Training Loss','Validation Loss'])\n",
    "plt.savefig('baseline_loss_epoch.png', transparent= False, bbox_inches= 'tight', dpi= 900)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = keras.preprocessing.image.load_img(\n",
    "    \"test.jpg\", target_size=(224,224)\n",
    ")\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_array = keras.preprocessing.image.img_to_array(img)\n",
    "img_array = np.expand_dims(img_array, 0)  # Create batch axis\n",
    "\n",
    "\n",
    "\n",
    "img_array = preprocess_input(img_array)\n",
    "\n",
    "res = model.predict(img_array)\n",
    "# print(res)\n",
    "res = np.argmax(res);\n",
    "print(res)\n",
    "\n",
    "\n",
    "\n",
    "# print(train_generator.class_indices)\n",
    "\n",
    "label_map = (list(train_generator.class_indices.keys()))\n",
    "\n",
    "print (label_map[res])\n",
    "# print(label_map.get(\"n02085936-Maltese_dog\"))\n",
    "# print(label_map)\n",
    "# print(label_map.keys())\n",
    "\n",
    "# print(train_generator.labels)\n",
    "\n",
    "# y_classes = keras.np_utils.probas_to_classes(res)\n",
    "# print(y_classes)\n",
    "\n",
    "# y_classes = res.argmax(axis=-1)\n",
    "# print(y_classes)\n",
    "\n",
    "\n",
    "# y_classes = keras.np_utils.probas_to_classes(res)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
